<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BGD、SGD和MBGD的一些区别</title>
    <url>/2017/09/12/BGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>本文主要讲解了 梯度下降(Batch gradient descent)，随机梯度下降(Stochastic gradient descent)，小批量梯度下降(Mini-batch gradient descent)在实现上的区别</p>
<a id="more"></a>


<h3 id="梯度下降-Batch-gradient-descent-–BGD"><a href="#梯度下降-Batch-gradient-descent-–BGD" class="headerlink" title="梯度下降(Batch gradient descent)–BGD"></a>梯度下降(Batch gradient descent)–BGD</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度下降(Batch gradient descent)--BGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x:  输入的x</span></span><br><span class="line"><span class="string">    :param y:  输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="comment"># 全部的值带入，计算 梯度</span></span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>





<h3 id="随机梯度下降-Stochastic-gradient-descent-–SGD"><a href="#随机梯度下降-Stochastic-gradient-descent-–SGD" class="headerlink" title="随机梯度下降(Stochastic gradient descent)–SGD"></a>随机梯度下降(Stochastic gradient descent)–SGD</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_false</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> batch_gradient_descent(x, y, learn_rate, epoches)</span><br><span class="line"><span class="comment"># 正确的随机梯度应该是这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_true</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        shufflle_data = np.column_stack((y, x))</span><br><span class="line">        np.random.shuffle(shufflle_data)</span><br><span class="line">        stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">        <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">        y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">        x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 随机之后的值，进行梯度计算</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>



<h3 id="小批量梯度下降-Mini-batch-gradient-descent-–MBGD"><a href="#小批量梯度下降-Mini-batch-gradient-descent-–MBGD" class="headerlink" title="小批量梯度下降(Mini-batch gradient descent)–MBGD"></a>小批量梯度下降(Mini-batch gradient descent)–MBGD</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mini_batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches, mini_length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :param mini_length: mini batch length</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 随机打乱----optional</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="comment"># 随机打乱数据  ----optional</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        <span class="comment"># 0-min_length， mini_length+1  2mini_length, ....... 一小段，一小段距离用于一次优化迭代</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(y), mini_length):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / mini_length</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / mini_length</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>


<h3 id="实验代码"><a href="#实验代码" class="headerlink" title="实验代码"></a>实验代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Date    : 2017/9/8</span></span><br><span class="line"><span class="comment"># @Author  : ryanbing (legotime@qq.com)</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">x = <span class="number">10</span> * rng.rand(<span class="number">500</span>)</span><br><span class="line">y = <span class="number">3</span> * x + <span class="number">2</span> + rng.randn(<span class="number">500</span>)</span><br><span class="line"><span class="comment"># plt.scatter(x, y)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># 找出 y = wx + b 中的w 和 b, 正确的应该是 w = 3, b = 2</span></span><br><span class="line"><span class="comment"># 我们在计算的时候其看成 y = WX 其中 W= [w, b], X = [x, 1].T</span></span><br><span class="line"><span class="comment"># 梯度下降(Batch gradient descent)--BGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x:  输入的x</span></span><br><span class="line"><span class="string">    :param y:  输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="comment"># 全部的值带入，计算 梯度</span></span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="comment"># 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_false</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, batch_gradient_descent(x, y, learn_rate, epoches)</span><br><span class="line"><span class="comment"># 正确的随机梯度应该是这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent_true</span>(<span class="params">x, y, learn_rate, epoches, stochastic_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        shufflle_data = np.column_stack((y, x))</span><br><span class="line">        np.random.shuffle(shufflle_data)</span><br><span class="line">        stochastic_count = int(len(y) * stochastic_rate)</span><br><span class="line">        <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">        y = shufflle_data[:stochastic_count, <span class="number">0</span>]</span><br><span class="line">        x = shufflle_data[:stochastic_count, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># 随机之后的值，进行梯度计算</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        m = len(y)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / m</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / m</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="comment"># 小批量梯度下降(Mini-batch gradient descent)--MBGD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mini_batch_gradient_descent</span>(<span class="params">x, y, learn_rate, epoches, mini_length</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param x: 输入的x</span></span><br><span class="line"><span class="string">    :param y: 输入的y</span></span><br><span class="line"><span class="string">    :param learn_rate: 学习率</span></span><br><span class="line"><span class="string">    :param epoches: 迭代次数</span></span><br><span class="line"><span class="string">    :param mini_length: mini batch length</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = datetime.datetime.now()</span><br><span class="line">    <span class="comment"># 随机打乱----optional</span></span><br><span class="line">    theta = np.array([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="comment"># 随机打乱数据  ----optional</span></span><br><span class="line">    shufflle_data = np.column_stack((y, x))</span><br><span class="line">    np.random.shuffle(shufflle_data)</span><br><span class="line">    <span class="comment"># 然后随机取一些数据进行梯度优化， 比如取随机100条数据</span></span><br><span class="line">    y = shufflle_data[:, <span class="number">0</span>]</span><br><span class="line">    x = shufflle_data[:, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoches):</span><br><span class="line">        <span class="comment"># 0-min_length， mini_length+1  2mini_length, ....... 一小段，一小段距离用于一次优化迭代</span></span><br><span class="line">        loss = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(y), mini_length):</span><br><span class="line">            loss[<span class="number">0</span>] = loss[<span class="number">0</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) * x[j, <span class="number">0</span>] / mini_length</span><br><span class="line">            loss[<span class="number">1</span>] = loss[<span class="number">1</span>] + (theta[<span class="number">0</span>] * x[j, <span class="number">0</span>] + theta[<span class="number">1</span>] * x[j, <span class="number">1</span>] - y[j]) / mini_length</span><br><span class="line">        <span class="comment"># 更新 theta</span></span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - learn_rate * loss[<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - learn_rate * loss[<span class="number">1</span>]</span><br><span class="line">    end_time = datetime.datetime.now()</span><br><span class="line">    <span class="keyword">return</span> end_time - start_time, theta</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contro_func</span>(<span class="params">func, **kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param func: 函数</span></span><br><span class="line"><span class="string">    :param kwargs:  func 中需要的参数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = kwargs.get(<span class="string">&#x27;x&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    y = kwargs.get(<span class="string">&#x27;y&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    learn_rate = kwargs.get(<span class="string">&#x27;learn_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    epoches = kwargs.get(<span class="string">&#x27;epoches&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    stochastic_rate = kwargs.get(<span class="string">&#x27;stochastic_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    mini_length = kwargs.get(<span class="string">&#x27;mini_length&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># change the value is args is not num</span></span><br><span class="line">    <span class="keyword">if</span> stochastic_rate <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> func(x, y, learn_rate, epoches, stochastic_rate)</span><br><span class="line">    <span class="keyword">if</span> mini_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> func(x, y, learn_rate, epoches, mini_length)</span><br><span class="line">    <span class="keyword">return</span> func(x, y, learn_rate, epoches)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_trend</span>():</span></span><br><span class="line">    <span class="comment"># 画出收敛的的图像和收敛对应的时间</span></span><br><span class="line">    rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">    x = <span class="number">10</span> * rng.rand(<span class="number">500</span>)</span><br><span class="line">    x = np.array([x, np.ones(<span class="number">500</span>)]).T</span><br><span class="line">    y = <span class="number">3</span> * x + <span class="number">2</span> + rng.randn(<span class="number">500</span>)</span><br><span class="line">    learn_rate = <span class="number">0.01</span></span><br><span class="line">    stochastic_rate = <span class="number">0.4</span></span><br><span class="line">    mini_length = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> [batch_gradient_descent, stochastic_gradient_descent_false,</span><br><span class="line">              stochastic_gradient_descent_true, mini_batch_gradient_descent]:</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="keyword">for</span> epoches <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>, <span class="number">100000</span>]:</span><br><span class="line">            tmp.append(contro_func(i, x=x, y=y, learn_rate=learn_rate, stochastic_rate=stochastic_rate,</span><br><span class="line">                                   mini_length=mini_length, epoches=epoches))</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># test(func=func, x=1, y=2, learn_rate=3, epoches=4, stochastic_rate=5)</span></span><br><span class="line">    <span class="comment"># print(batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100000))</span></span><br><span class="line">    <span class="comment"># [ 1.14378512 0.17288215]</span></span><br><span class="line">    <span class="comment"># [ 3.18801281 0.50870366]</span></span><br><span class="line">    <span class="comment"># [ 3.18602557 0.806018 ]</span></span><br><span class="line">    <span class="comment"># [ 3.03276102 1.84267445]</span></span><br><span class="line">    <span class="comment"># [ 3.01449298 1.96623647]</span></span><br><span class="line">    <span class="comment"># [ 3.01449298 1.96623647]</span></span><br><span class="line">    <span class="comment"># print(stochastic_gradient_descent_false(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100,stochastic_rate=0.4))</span></span><br><span class="line">    <span class="comment"># [ 1.11939055 0.16949282]</span></span><br><span class="line">    <span class="comment"># [ 3.19877639 0.50404936]</span></span><br><span class="line">    <span class="comment"># [ 3.20921332 0.78698163]</span></span><br><span class="line">    <span class="comment"># [ 3.04720128 1.82412805]</span></span><br><span class="line">    <span class="comment"># [ 3.01920995 1.89883629]</span></span><br><span class="line">    <span class="comment"># [ 2.98281143 2.15226071]</span></span><br><span class="line">    <span class="comment"># print(stochastic_gradient_descent_true(np.array([x, np.ones(50000)]).T, y, learn_rate=0.01, epoches=1000,stochastic_rate=1))</span></span><br><span class="line">    <span class="comment"># print(mini_batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100, mini_length=10))</span></span><br><span class="line">    <span class="comment"># [ 0.94630842  0.14845568]</span></span><br><span class="line">    <span class="comment"># [ 0.8811451   0.15444328]</span></span><br><span class="line">    <span class="comment"># [ 3.18337012  0.51049921]</span></span><br><span class="line">    <span class="comment"># [ 3.14833317  0.79174635]</span></span><br><span class="line">    <span class="comment"># [ 3.03507147  1.87931184]</span></span><br></pre></td></tr></table></figure>


]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《HOW-TO-DEFI》读书笔记</title>
    <url>/2020/08/27/%E3%80%8AHOW-TO-DEFI%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>本文主要是记录阅读《HOW-TO-DEFI》的一些读书笔记，只用于个人回顾记忆</p>
<a id="more"></a>


<h3 id="中心化-amp-去中心化金融"><a href="#中心化-amp-去中心化金融" class="headerlink" title="中心化 &amp; 去中心化金融"></a>中心化 &amp; 去中心化金融</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">支付与清算系统</span><br><span class="line">	1.支付 &amp; 清算系统(汇款)</span><br><span class="line">		跨国家之间各种繁琐的问题:时间长和多种手续费,同时可能要提供证明文件和反洗钱的法律，隐私等问题</span><br><span class="line">		比如美国人想澳大利人转：美国人要在银行进行汇率的转化，国际电汇汇出手续费以及国际电 汇汇入手续费，同时什么时候收到收时间，地点的限制</span><br><span class="line">	2.可获取性</span><br><span class="line">		无国界、无审查、无障碍金融产品,任何一个人都可以进行注册，</span><br><span class="line">	3.中心化 &amp; 透明度</span><br><span class="line">		开源，便于审计和提升透明度</span><br><span class="line">去中心化金融 vs 传统金融</span><br><span class="line">	去中心化金融(去中心化金融(DeFi)是一场能够让用户在无需依靠中心化实体的 情况下使用诸如借贷和交易等金融服务的运动)</span><br><span class="line">		让每个人都可以获取金融服务，而 不需接受任何形式的审查</span><br><span class="line">	传统金融</span><br><span class="line">DeFi分类</span><br><span class="line">	中心化分类</span><br><span class="line">		1. 中心化</span><br><span class="line">			○ 特征:托管，中心化喂价，中心化地决定利率，追加保证金时中心化地注入流动性</span><br><span class="line">			○ 例子:Salt, BlockFi, Nexo, 和 Celsius</span><br><span class="line">		2. 半去中心化(具有上述一个或多个而非全部的特征)</span><br><span class="line">			○ 特征:非托管，去中心化喂价，无须许可地发起保证金追 加，无须许可地调整流动性，去中心化地决定利率，去中心化平台开发/升级</span><br><span class="line">			○ 例子:Compound, MakerDAO, dYdX, 和 bZx</span><br><span class="line">		3. 完全去中心化</span><br><span class="line">			○ 特征:每个组件都是去中心化的</span><br><span class="line">			○ 例子:目前还没有完全去中心化的DeFi协议</span><br><span class="line">	DeFi 的主要类别</span><br><span class="line">		稳定币：</span><br><span class="line">			锚定诸如美元等稳定 资产的稳定币</span><br><span class="line">		借贷和放贷：</span><br><span class="line">			允许任何人抵押其数字资产 ，然后利用抵押资产获得贷款。或者进行放贷</span><br><span class="line">		交易所：</span><br><span class="line">			去中心化交易</span><br><span class="line">		衍生品:</span><br><span class="line">			来源于股票、商品、货币、指数、债券或利 率等其它标的资产的合约</span><br><span class="line">		基金管理(监管你的资产并管理其现金流以产生投资回报的过程):</span><br><span class="line">			主动型:</span><br><span class="line">				有一个管理团队负责投资 决策</span><br><span class="line">			被动型:</span><br><span class="line">				无团队，尽可能接近 某一特定基准的表现</span><br><span class="line">		彩票：</span><br><span class="line">			设定的时间间隔将赚取的利息交给随机选中的中奖者</span><br><span class="line">		支付：</span><br><span class="line">			引入对支付模式的新思考</span><br><span class="line">		保险：</span><br></pre></td></tr></table></figure>

<h3 id="初探DEFI"><a href="#初探DEFI" class="headerlink" title="初探DEFI"></a>初探DEFI</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">什么是智能合约</span><br><span class="line">	智能合约按照“如果这样，那么那样”的原理工作</span><br><span class="line">以太币(Ether/ETH)是什么?</span><br><span class="line">	以太币是以太坊区块链的原生数字货币</span><br><span class="line">Gas是什么?</span><br><span class="line">	在以太坊上，所有交易和合约执行都需要支付少量费用。这笔费用被称为Gas</span><br><span class="line">去中心化应用(Dapp)是什么?</span><br><span class="line">	在以太坊网路下，Dapp是通过使用智能合约来与区块链进行交互的接口。从前端来看，Dapp的外观和操作类似于常规的Web应用和移动应用，只是它们是以不同的方式与区块链进行交互</span><br><span class="line">Dapp有何优势?</span><br><span class="line">	● 不变性:一旦信息保存在区块链上，任何人都不能更改;</span><br><span class="line">	● 防篡改:发布在区块链上的智能合约不能在区块链上的其它参与者不知情的情况下被篡改;</span><br><span class="line">	● 透明性:智能合约驱动的Dapp是公开可审计的;</span><br><span class="line">	● 可用性:只要以太坊网络保持活性，在其之上搭建的Dapp将保持活性和可用性。</span><br><span class="line">Dapp有何劣势</span><br><span class="line">	● 不变性:智能合约由人所编写，因此人为错误是不可避免的，而不可变的智能合约有可能会将错误放大;</span><br><span class="line">	● 透明性:公开可审计的智能合约也能成为黑客攻击的媒介，因为黑客可以通过查看代码来发现合约漏洞;</span><br><span class="line">	● 可扩展性:在大多数情况下，Dapp的带宽受限于其所在的区块链。</span><br><span class="line">以太坊还能做什么?</span><br><span class="line">	创建去中心化自治组织 (DAO) 或发行其它加密货币。</span><br><span class="line">DAO</span><br><span class="line">	DAO是一个完全自治的组织，它不由个体管理，而是通过代码进行管理</span><br><span class="line">发行其它加密货币</span><br><span class="line">	ERC-20</span><br><span class="line">		可互换代币，意味着代币间是可互换的并具有相同的价值</span><br><span class="line">	ERC-721</span><br><span class="line">		不可互换代币，意味着代币是唯一的且不可互换的</span><br><span class="line">以太坊钱包</span><br><span class="line">	钱包是连接区块链网络的用户友好接口。钱包管理着你的私钥，而私钥是你加密货币保险库锁的钥匙。钱包让你能够接收、存储和发送加密货币</span><br></pre></td></tr></table></figure>

<h3 id="DEFI方向"><a href="#DEFI方向" class="headerlink" title="DEFI方向"></a>DEFI方向</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">去中心稳定币</span><br><span class="line">    (占比80%)Tether(USDT)-&gt; 1 usdt 持有1美元储备金</span><br><span class="line">    DAI -&gt; 通过加密货币抵押生成(通过由一个去中心化自治组织表决的协议和智能合约来实 现与1美元锚定)</span><br><span class="line">        发行平台：marker(发行在eth的智能合约，有Dai, Sai以及治理代币Marker)</span><br><span class="line">            Sai: 单抵押Dai，仅由以太币(ETH)作为抵押品背 书。</span><br><span class="line">                思想：比如ETH抵押率为 150%， 如果1个eth价值150美元，你可以拿到 100美元的DAI作为可用币种</span><br><span class="line">            Dai: 多抵押Dai。Dai目前由以 太币(ETH)和注意力币(BAT)作为抵押品背书，并计划在未来增 加其它类型资产作为抵押品。</span><br><span class="line"></span><br><span class="line">去中心化借贷</span><br><span class="line">    Compound Finance(一个DiFI借贷协议)</span><br><span class="line">        Compound 有一个代币 cToken</span><br><span class="line">            放款人: 购买同价值的cToken, 然后一段时间之后cToken升值了，然后卖掉cToken 作为你的现在的价值</span><br><span class="line">            借款人: 按照你资产的抵押率进行抵押，在收取一点费用(清算)情况下</span><br><span class="line">                抵押品升值</span><br><span class="line">                    抵押率就会上升，你可以获得更多的贷款如果你想要</span><br><span class="line">                抵押品贬值</span><br><span class="line">                    部分变卖，收取5%的手续费</span><br><span class="line">        清算：</span><br><span class="line">            当抵押品提供的价值低于借款时，清算就会发生。这是为了确保金提取和借出总是有超额的兑现能力，同时保护借款人免受违约风险。目前的清算手续费为5%。</span><br><span class="line"></span><br><span class="line">去中心化交易所(DEX)</span><br><span class="line">    类型</span><br><span class="line">        基于订单簿的DEX(与CEX类似)</span><br><span class="line">            交易流程： 用户按照限价或者市价提交买卖订单，和CEX区别，交易资产保存在自己钱包</span><br><span class="line">        基于流动性池的DEX(本质-&gt; 智能合约中的代币准备金，价格由算法确定-&gt; 大量交易会上涨价值)</span><br><span class="line"></span><br><span class="line">去中心化衍生品</span><br><span class="line">	衍生品是一种合约，其价值来自于其他对标资产，例如股票、商品 、货币、指数、债券或利率</span><br><span class="line">	</span><br><span class="line">去中心化资金管理</span><br><span class="line">	资金管理是通过资产监督和现金流管理实现投资回报的过程</span><br><span class="line"></span><br><span class="line">去中心化彩票</span><br><span class="line">	优势</span><br><span class="line">		1. 资金不通过中间 人或经纪人，而是由经过审计的智能合约进行处理。资金也没有锁 定期，这意味着它们可以随时被提取出来。</span><br><span class="line">		2. 不受地域限制</span><br><span class="line"></span><br><span class="line">去中心化支付</span><br><span class="line">	目的: 实现更便宜、更快速的交易、定时转账、按条件转账以及 标准化的发票格式等等</span><br><span class="line"></span><br><span class="line">去中心化保险</span><br><span class="line">	DEFI用户面临的风险点</span><br><span class="line">			1. 技术风险:智能合约存在漏洞，遭到安全性攻击; </span><br><span class="line">			2. 流动性风险:类似Compound平台的流动性耗尽; </span><br><span class="line">			3. 密钥管理风险:平台的主私钥可能被盗取</span><br><span class="line">	承保机制如何运作</span><br><span class="line">			入保. 选择承保期限和承保金额</span><br><span class="line">				定价</span><br><span class="line">					智能合约的特性。比如包括智能合约中存储的资金量、已处 理的交易等</span><br><span class="line">					承保金额</span><br><span class="line">					承保期限</span><br><span class="line">					风险评估师针对智能合约的资金抵押</span><br><span class="line">				如何购买：</span><br><span class="line">					1. 指定要承保的智能合约地址。</span><br><span class="line">					2. 指定承保金额(ETH或DAI)和承保期限。</span><br><span class="line">					3. 生成报价并使用Metamask进行交易。</span><br><span class="line">			保险: 启动流程-&gt; 评估 -&gt; 支付</span><br><span class="line">	</span><br><span class="line">DEFI仪表盘</span><br><span class="line">	仪表盘(Dashboard)是一个将所有DeFi活动进行汇总简单的平台 。这是一个实用工具，可以显示和跟踪资产在不同DeFi协议中的位 置。它能将您的资产分为不同的类别，如存款、债务和投资</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
</search>
